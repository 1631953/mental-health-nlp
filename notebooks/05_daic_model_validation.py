# MODEL VALIDATION USING DAIC-WOZ DATASET
# -*- coding: utf-8 -*-
"""05_daic_model_validation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PJV6XqMQ00R712_Nc7mXwLmYH6ypCLkS
"""

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score
import plotly.figure_factory as ff
from textblob import TextBlob
import numpy as np
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from xgboost import XGBClassifier
import joblib
import seaborn as sns
from scipy.sparse import hstack
from sklearn.utils.multiclass import unique_labels
from sklearn.preprocessing import LabelEncoder, StandardScaler

df = pd.read_csv("clean_DAICWOZ.csv")
df = df[df['status']=='Depression']

"""### *Lbl Enc import*

#### *Clean env*
"""

!pip install --upgrade --force-reinstall numpy

# In a clean environment
import joblib
import json
from sklearn.preprocessing import LabelEncoder

# Load original encoder
lbl_enc = joblib.load('label_encoder.joblib')

# Convert to dict
encoder_dict = {
    'classes_': lbl_enc.classes_.tolist()
}

# Save to JSON
with open('label_encoder.json', 'w') as f:
    json.dump(encoder_dict, f)

# Encode
y_true = lbl_enc.transform(df['status'])

"""#### *Gensim compatible env*"""

import json
import numpy as np
from sklearn.preprocessing import LabelEncoder

# Load encoder manually
with open('label_encoder.json', 'r') as f:
    data = json.load(f)

lbl_enc = LabelEncoder()
lbl_enc.classes_ = np.array(data['classes_'])

# Encode
y_true = lbl_enc.transform(df['status'])

"""### *Evaluate model*"""

def evaluate_model(model, X, y, class_names, model_name='Model', sample_weights=None):
    from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix

    # Predict
    y_pred = model.predict(X)

    # Metrics
    acc = accuracy_score(y, y_pred)
    f1_macro = f1_score(y, y_pred, average='macro')
    f1_micro = f1_score(y, y_pred, average='micro')

    print(f"\n--- Results for {model_name} ---")
    print(f"Accuracy      : {acc:.4f}")
    print(f"F1 Macro Avg  : {f1_macro:.4f}")
    print(f"F1 Micro Avg  : {f1_micro:.4f}\n")

    print("Classification Report:")

    all_labels = list(range(len(class_names)))

    print(classification_report(
        y, y_pred,
        labels=all_labels,
        target_names=class_names,
        digits=3,
        zero_division=0
    ))

    # Confusion matrix
    cm = confusion_matrix(y, y_pred, labels=all_labels)

    # Compact style
    fig, ax = plt.subplots(1, 1, figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',
                xticklabels=class_names, yticklabels=class_names, ax=ax)
    ax.set_xlabel('Predicted')
    ax.set_ylabel('Actual')
    ax.set_title(f'Confusion Matrix for {model_name}')
    plt.tight_layout()
    plt.show()

    return acc, f1_macro, f1_micro

"""# **TF-IDF**

#### *Vectorize*
"""

# Load vectorizer
vectorizer = joblib.load('tfidf_vectorizer.joblib')

# Vectorize text
X_tfidf = vectorizer.transform(df['tokens'])

"""## *TF-IDF only*"""

# Load model
name = 'Logistic Regression + TF-IDF Model'
model = joblib.load('logreg_tfidf_model.joblib')

_, _, _, y_pred_tfidf = evaluate_model(model, X_tfidf, y_true, class_names=lbl_enc.classes_)

import numpy as np

print("Predicted label indices:", np.unique(y_pred_tfidf))
print("Predicted label names:", lbl_enc.classes_[np.unique(y_pred_tfidf)])

"""## *Combined TF-IDF*"""

# Load model
name = 'XGB + TF-IDF + Word Count'
model = joblib.load('XGB_tfidf_comb_model.joblib')

# Extract word_count and combine
scaler = StandardScaler()
word_count_scaled = scaler.fit_transform(df[['word_count']].values.astype(np.float32))

X_tfidf_combined = hstack([X_tfidf, word_count_scaled])


# Use your existing model (if trained with same combined feature format)
_, _, _, y_pred_combined = evaluate_model(
    model,
    X_tfidf_combined,
    y_true,
    class_names=lbl_enc.classes_,  # full label set, even if some are missing
    model_name=name
)

"""# **Word2Vec**

#### *Vectorize*
"""

!pip install --force-reinstall numpy==1.24.4
!pip install --upgrade gensim

from gensim.models import Word2Vec
import numpy as np

# Load W2V model
w2v_model = Word2Vec.load('w2v_vectorizer.bin')

# Average Word2Vec vectorizer
def document_vector(doc, w2v_model):
    doc = [word for word in doc if word in w2v_model.wv]
    if len(doc) == 0:
        return np.zeros(w2v_model.vector_size)
    return np.mean(w2v_model.wv[doc], axis=0)

# Vectorize using W2V
X_w2v = np.array([document_vector(tokens, w2v_model) for tokens in df['tokens']])

"""## *W2V only*"""

# Load model and vectorizer
name = 'Random Forest + Word2Vec Model'
model = joblib.load('rf_w2v_model.joblib')

_, _, _, y_pred_w2v = evaluate_model(model, X_w2v, y_true, class_names=lbl_enc.classes_)

"""## *Combined W2V*"""

# Load model and vectorizer
name = 'Random Forest + Word2Vec + Word Count'
model = joblib.load('rf_w2v_combined_model.joblib')

scaler = StandardScaler()
X_w2v_combined = scaler.fit_transform(np.hstack([X_w2v, df[['word_count']].values.astype(np.float32)]))

# Use your existing model (if trained with same combined feature format)
_, _, _, y_pred_combined = evaluate_model(
    model,
    X_w2v_combined,
    y_true,
    class_names=lbl_enc.classes_,  # full label set, even if some are missing
    model_name=name
)

"""# **BERT**

#### *Vectorize*
"""

df = pd.read_csv("daic_woz_PrePreprocessing.csv")
df = df[df['status']=='Depression']

import re

def clean_for_bert(text):
    text = re.sub(r'<.*?>+', '', text)
    text = re.sub(r'http[s]?://\S+', '', text)
    text = re.sub(r'\n', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

df['cleaned_for_bert'] = df['statement'].astype(str).apply(clean_for_bert)

from sentence_transformers import SentenceTransformer
from tqdm import tqdm

bert_model = SentenceTransformer('all-MiniLM-L6-v2')

# Generate embeddings
X_bert = np.array([bert_model.encode(text) for text in tqdm(df['cleaned_for_bert'])])

X_bert_combined = np.hstack([X_bert, df[['word_count']].values.astype(np.float32)])

"""## *BERT only*

###*Embedding Visualisation*

#### *Get original Kaggle dataset representations*
"""

import pandas as pd
dfKaggle = pd.read_pickle("df_BERT_embeddings.pkl")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.preprocessing import LabelEncoder

# Ensure embeddings are in the correct format (shape: [n_samples, embedding_dim])
bert_vectors = np.vstack(dfKaggle['bert_embedding'].values)

# Reduce dimensionality with t-SNE
X_2d_bert = TSNE(n_components=2, random_state=42).fit_transform(bert_vectors)

# Encode labels (for coloring)
le = LabelEncoder()
labels = le.fit_transform(df['status'])

df_vis = pd.DataFrame(X_2d_bert, columns=['x', 'y'])
df_vis['label'] = df['status'].values

"""#### *Filter out original Depression samples*"""

# Filter out original Depression samples
df_vis_filtered = df_vis[df_vis['label'] != 'Depression']

"""#### *Get DAIC-WOZ Depression embeddings*"""

# Create new dataframe for new depression data
from sklearn.manifold import TSNE

# Reduce dimensionality with t-SNE
DAICWOZ_X_2d_bert = TSNE(n_components=2, random_state=42).fit_transform(X_bert)

# Encode labels (for coloring)
le = LabelEncoder()
labels = le.fit_transform(df['status'])

# Plot
new_depression_embeddings = pd.DataFrame(DAICWOZ_X_2d_bert, columns=['x', 'y'])
new_depression_embeddings['label'] = df['status'].values


new_depression_df = pd.DataFrame(new_depression_embeddings, columns=['x', 'y'])
new_depression_df['label'] = 'New_Depression'

"""#### *Combine and Plot*"""

# Combine the datasets
combined_df = pd.concat([df_vis_filtered, new_depression_df], ignore_index=True)

# Define the color palette to match your original
color_palette = {
    'Anxiety': '#1f77b4',     # Blue
    'Normal': '#ff7f0e',      # Orange
    'Suicidal': '#d62728',    # Red
    'Bipolar': '#9467bd',     # Purple
    'New_Depression': '#2ca02c'  # Same green as original Depression
}

# Plot
plt.figure(figsize=(12, 7))
sns.scatterplot(
    data=combined_df,
    x='x', y='y',
    hue='label',
    palette=color_palette,
    style=combined_df['label'].apply(lambda x: 'DAIC-WOZ samples' if x == 'New_Depression' else 'Kaggle Dataset samples'),
    alpha=0.7
)
plt.title("t-SNE Projection with New Depression Samples Highlighted")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
# Get current legend entries
handles, labels = plt.gca().get_legend_handles_labels()
# Replace 'New_Depression' with 'Depression' in the legend
labels = ['Depression' if lbl == 'New_Depression' else lbl for lbl in labels]
# Update legend
plt.legend(handles=handles, labels=labels, title="Mental Health Category")
plt.grid(True)
plt.show()

"""### *Model prediction*"""

# Load model
name = 'XGB + BERT Model'
model = joblib.load('XGB_bert_model.joblib')

acc, f1_macro, f1_micro = evaluate_model(model, X_bert, y_true, class_names=lbl_enc.classes_, model_name='XGB + BERT Model')

# Load model
name = 'XGB + BERT Model'
model = joblib.load('XGB_bert_model.joblib')

_, _, _, y_pred_bert = evaluate_model(model, X_bert, y_true, class_names=lbl_enc.classes_)

"""## *Combined BERT*"""

# Load model and vectorizer
name = 'XGB + BERT + Word Count'
model = joblib.load('XGB_bert_combined_model.joblib')

# Use your existing model (if trained with same combined feature format)
_, _, _, y_pred_combined = evaluate_model(
    model,
    X_bert_combined,
    y_true,
    class_names=lbl_enc.classes_,  # full label set, even if some are missing
    model_name=name
)

X_bert = np.array([bert_model.encode(text) for text in df['cleaned_for_bert']])
print(X_bert.shape)

# Should be: (n_samples, 1000) if model expects 1001 total features
df['word_count'] = df['cleaned_for_bert'].apply(lambda x: len(x.split()))
X_bert_combined = np.hstack([X_bert, df[['word_count']].values.astype(np.float32)])
print(X_bert_combined.shape)

"""# **MISTRAL**

#### *Vectorize*
"""

df = pd.read_csv("daic_woz_PrePreprocessing.csv")
df = df[df['status']=='Depression']

import re

def clean_for_mistral(text):
    text = re.sub(r'<.*?>+', '', text)
    text = re.sub(r'http[s]?://\S+', '', text)
    text = re.sub(r'\n', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

df['cleaned_for_mistral'] = df['statement'].astype(str).apply(clean_for_mistral)

from huggingface_hub import login

login()

import torch
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm
import numpy as np

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name = "mistralai/Mistral-7B-Instruct-v0.1"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)
model.to(device)
model.eval()

def get_mistral_embedding(text):
    with torch.no_grad():
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        outputs = model(**inputs)
        hidden_states = outputs.last_hidden_state.squeeze(0)
        embedding = hidden_states.mean(dim=0).cpu().numpy().astype(float)
    return embedding

# Generate embeddings
X_mistral = np.array([get_mistral_embedding(text) for text in tqdm(df['cleaned_for_mistral'])])

X_mistral_combined = np.hstack([X_mistral, df[['word_count']].values.astype(np.float32)])

"""## *MISTRAL only*

###*Embedding Visualisation*

#### *Get original Kaggle dataset representations*
"""

import pandas as pd
dfKaggle = pd.read_pickle("df_MISTRAL_embeddings.pkl")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.preprocessing import LabelEncoder

# Ensure embeddings are in the correct format (shape: [n_samples, embedding_dim])
bert_vectors = np.vstack(dfKaggle['bert_embedding'].values)

# Reduce dimensionality with t-SNE
X_2d_bert = TSNE(n_components=2, random_state=42).fit_transform(bert_vectors)

# Encode labels (for coloring)
le = LabelEncoder()
labels = le.fit_transform(df['status'])

df_vis = pd.DataFrame(X_2d_bert, columns=['x', 'y'])
df_vis['label'] = df['status'].values

"""#### *Filter out original Depression samples*"""

# Filter out original Depression samples
df_vis_filtered = df_vis[df_vis['label'] != 'Depression']

"""#### *Get DAIC-WOZ Depression embeddings*"""

# Create new dataframe for new depression data
from sklearn.manifold import TSNE

# Reduce dimensionality with t-SNE
DAICWOZ_X_2d_bert = TSNE(n_components=2, random_state=42).fit_transform(X_bert)

# Encode labels (for coloring)
le = LabelEncoder()
labels = le.fit_transform(df['status'])

# Plot
new_depression_embeddings = pd.DataFrame(DAICWOZ_X_2d_bert, columns=['x', 'y'])
new_depression_embeddings['label'] = df['status'].values


new_depression_df = pd.DataFrame(new_depression_embeddings, columns=['x', 'y'])
new_depression_df['label'] = 'New_Depression'

"""#### *Combine and Plot*"""

# Combine the datasets
combined_df = pd.concat([df_vis_filtered, new_depression_df], ignore_index=True)

# Define the color palette to match your original
color_palette = {
    'Anxiety': '#1f77b4',     # Blue
    'Normal': '#ff7f0e',      # Orange
    'Suicidal': '#d62728',    # Red
    'Bipolar': '#9467bd',     # Purple
    'New_Depression': '#2ca02c'  # Same green as original Depression
}

# Plot
plt.figure(figsize=(12, 7))
sns.scatterplot(
    data=combined_df,
    x='x', y='y',
    hue='label',
    palette=color_palette,
    style=combined_df['label'].apply(lambda x: 'DAIC-WOZ samples' if x == 'New_Depression' else 'Kaggle Dataset samples'),
    alpha=0.7
)
plt.title("t-SNE Projection with New Depression Samples Highlighted")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
# Get current legend entries
handles, labels = plt.gca().get_legend_handles_labels()
# Replace 'New_Depression' with 'Depression' in the legend
labels = ['Depression' if lbl == 'New_Depression' else lbl for lbl in labels]
# Update legend
plt.legend(handles=handles, labels=labels, title="Mental Health Category")
plt.grid(True)
plt.show()

"""### *Model prediction*"""

# Load model
name = 'XGB + MISTRAL Model'
model = joblib.load('XGB_mistral_model.joblib')

_, _, _, y_pred_bert = evaluate_model(model, X_mistral, y_true, class_names=lbl_enc.classes_)

"""## *Combined W2V*"""

# Load model and vectorizer
name = 'XGB + MISTRAL + Word Count'
model = joblib.load('XGB_mistral_combined_model.joblib')

# Use your existing model (if trained with same combined feature format)
_, _, _, y_pred_combined = evaluate_model(
    model,
    X_mistral_combined,
    y_true,
    class_names=lbl_enc.classes_,  # full label set, even if some are missing
    model_name=name
)
