# -*- coding: utf-8 -*-
"""03_modeling_embeddings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CVzrTmmfY108xIRD47lPyVK6V8jj9Q9s

Let's evaluate:
*   4 vectorization techniques: TF-IDF, Word2Vec, BERT, Mistral
*   5 classifiers: LogisticRegression, RandomForest, SVM, Naive Bayes, XGBoost
* 5 training strategies per vectorizer:
  1. Plain text
  2. Text + word count
  3. Manual class weights
  4. SMOTE (skip for TF-IDF)
  5. Random oversampling
"""

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import plotly.figure_factory as ff
from textblob import TextBlob
import numpy as np
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from xgboost import XGBClassifier

df = pd.read_csv("df_PostPreprocessing_5classes.csv")

from sklearn.preprocessing import LabelEncoder
# Splitting the data
X = df[['tokens','word_count']]
y = df['status']

lbl_enc = LabelEncoder()
y = lbl_enc.fit_transform(y.values)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101, stratify=y)

"""## Evaluate model function"""

from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

def evaluate_model(model, X_train, y_train, X_test, y_test, class_names, model_name='Model', sample_weights=None):
    # Train with optional sample weights
    if sample_weights is not None:
        model.fit(X_train, y_train, sample_weight=sample_weights)
    else:
        model.fit(X_train, y_train)

    # Predict
    y_pred = model.predict(X_test)

    # Metrics
    acc = accuracy_score(y_test, y_pred)
    f1_macro = f1_score(y_test, y_pred, average='macro')
    f1_micro = f1_score(y_test, y_pred, average='micro')

    # Print results
    print(f"\n--- Results for {model_name} ---")
    print(f"Accuracy      : {acc:.4f}")
    print(f"F1 Macro Avg  : {f1_macro:.4f}")
    print(f"F1 Micro Avg  : {f1_micro:.4f}\n")

    print("Classification Report:")
    print(classification_report(y_test, y_pred, target_names=class_names, digits=3))

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',
                xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix for {model_name}')
    plt.show()

    cm = confusion_matrix(y_test, y_pred)

    return acc, f1_macro, f1_micro, cm

"""## Comparison

### Function
"""

def run_experiments(
    base_model,
    oversampled_model,
    model_name_base,
    X_train_vectorised, X_test_vectorised,
    X_train_combined, X_test_combined,
    y_train, y_test,
    class_names,
    manual_weights=False,
    perform_smote=True,
    try_combined=True
):
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    from sklearn.utils.class_weight import compute_class_weight
    from imblearn.over_sampling import RandomOverSampler, SMOTE

    results = []
    conf_matrices = []  # NEW: Store all confusion matrices

    # Sample weights
    if manual_weights:
        classes = np.unique(y_train)
        class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)
        weight_dict = {cls: weight for cls, weight in zip(classes, class_weights)}
        sample_weights = np.array([weight_dict[label] for label in y_train])
    else:
        sample_weights = None

    # 1. TF-IDF only
    name = f'{model_name_base} + TF-IDF'
    acc, f1_macro, f1_micro, cm = evaluate_model(
        base_model, X_train_vectorised, y_train, X_test_vectorised, y_test,
        class_names, model_name=name, sample_weights=sample_weights
    )
    results.append((name, acc, f1_macro, f1_micro))
    conf_matrices.append((name, cm))

    if try_combined:
        # 2. Combined features
        name = f'{model_name_base} + Combined'
        acc, f1_macro, f1_micro, cm = evaluate_model(
            base_model, X_train_combined, y_train, X_test_combined, y_test,
            class_names, model_name=name, sample_weights=sample_weights
        )
        results.append((name, acc, f1_macro, f1_micro))
        conf_matrices.append((name, cm))

        # 3. ROS
        ros = RandomOverSampler(random_state=101)
        X_train_ros, y_train_ros = ros.fit_resample(X_train_combined, y_train)
        name = f'{model_name_base} + Combined + ROS'
        acc, f1_macro, f1_micro, cm = evaluate_model(
            oversampled_model, X_train_ros, y_train_ros, X_test_combined, y_test,
            class_names, model_name=name
        )
        results.append((name, acc, f1_macro, f1_micro))
        conf_matrices.append((name, cm))

        # 4. SMOTE
        if perform_smote:
            smote = SMOTE(random_state=101)
            X_train_smote, y_train_smote = smote.fit_resample(X_train_combined, y_train)
            name = f'{model_name_base} + Combined + SMOTE'
            acc, f1_macro, f1_micro, cm = evaluate_model(
                oversampled_model, X_train_smote, y_train_smote, X_test_combined, y_test,
                class_names, model_name=name
            )
            results.append((name, acc, f1_macro, f1_micro))
            conf_matrices.append((name, cm))
    else:
        # TF-IDF + ROS
        ros = RandomOverSampler(random_state=101)
        X_train_ros, y_train_ros = ros.fit_resample(X_train_vectorised, y_train)
        name = f'{model_name_base} + TF-IDF + ROS'
        acc, f1_macro, f1_micro, cm = evaluate_model(
            oversampled_model, X_train_ros, y_train_ros, X_test_vectorised, y_test,
            class_names, model_name=name
        )
        results.append((name, acc, f1_macro, f1_micro))
        conf_matrices.append((name, cm))

        # TF-IDF + SMOTE
        if perform_smote:
            smote = SMOTE(random_state=101)
            X_train_smote, y_train_smote = smote.fit_resample(X_train_vectorised, y_train)
            name = f'{model_name_base} + TF-IDF + SMOTE'
            acc, f1_macro, f1_micro, cm = evaluate_model(
                oversampled_model, X_train_smote, y_train_smote, X_test_vectorised, y_test,
                class_names, model_name=name
            )
            results.append((name, acc, f1_macro, f1_micro))
            conf_matrices.append((name, cm))

    # --- Results Table ---
    df_results = pd.DataFrame(results, columns=["Setup", "Accuracy", "F1 Macro", "F1 Micro"])
    display(df_results)

    # --- Horizontal Bar Plot ---
    df_results.set_index("Setup")[["Accuracy", "F1 Macro", "F1 Micro"]].plot(
        kind="barh", figsize=(12, 6), width=0.75
    )
    plt.title("Model Performance Comparison", fontsize=14)
    plt.xlabel("Score", fontsize=12)
    plt.legend(loc='lower right')
    plt.tight_layout()
    plt.grid(axis='x', linestyle='--', alpha=0.6)
    plt.show()

    # --- Plot All Confusion Matrices ---
    n = len(conf_matrices)
    fig, axes = plt.subplots(1, n, figsize=(5 * n, 4))
    if n == 1:
        axes = [axes]

    for ax, (title, cm) in zip(axes, conf_matrices):
        sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',
                    ax=ax, xticklabels=class_names, yticklabels=class_names)
        ax.set_title(title)
        ax.set_xlabel("Predicted")
        ax.set_ylabel("Actual")

    plt.tight_layout()
    plt.show()

"""# **TF-IDF**

### Vectorising & Combining
"""

vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
#vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=50000)
X_train_tfidf = vectorizer.fit_transform(X_train['tokens'])
X_test_tfidf = vectorizer.transform(X_test['tokens'])

# 2. Extract numerical features
X_train_num = X_train[['word_count']].values
X_test_num = X_test[['word_count']].values

# 3. Combine TF-IDF features with numerical features
from scipy.sparse import hstack  # To combine sparse matrices
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_wc = scaler.fit_transform(X_train[['word_count']])
X_test_wc = scaler.transform(X_test[['word_count']])

X_train_combined = hstack([X_train_tfidf, X_train_wc])
X_test_combined = hstack([X_test_tfidf, X_test_wc])

# Normalizinf word_count is only NECESSARY for models that rely on distances:
# - LogisticRegression
# - SVM
# (Not XGBoost nor RF)

"""### Over Sampling (Data Augmentation)

#### SMOTE

X_train_combined is a sparse matrix (TF-IDF) + a dense matrix (word_count).

SMOTE does NOT support sparse matrices or properly mixed-type data.

Under the hood, it tries to interpolate between samples, but interpolation doesn't make sense in sparse space — leading to invalid or collapsed samples.

**SMOTE should only be applied to dense, numeric, low-dimensional data** so, to try SMOTE, we should reduce sparsity with TruncatedSVD before SMOTE.

## Logistic Regression

#### *Implementations*
"""

from sklearn.linear_model import LogisticRegression

# Model for original and weighted training
model_base = LogisticRegression(solver='liblinear', penalty='l1', C=10, class_weight='balanced', random_state=101)

# Model for oversampled training (no class_weight)
model_oversampled = LogisticRegression(solver='liblinear', penalty='l1', C=10, random_state=101)

run_experiments(
    base_model=model_base,
    oversampled_model=model_oversampled,
    model_name_base="LogReg",
    X_train_vectorised=X_train_tfidf,
    X_test_vectorised=X_test_tfidf,
    X_train_combined=X_train_combined,
    X_test_combined=X_test_combined,
    y_train=y_train,
    y_test=y_test,
    class_names=lbl_enc.classes_,
    perform_smote=False
)

# We do not manually compute class weights because of the param: class_weight='balanced'

"""#### *Best TF-IDF only model*"""

import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Train Logistic Regression model
model_tfidf = LogisticRegression(
    solver='liblinear',
    penalty='l1',
    C=10,
    class_weight='balanced',
    random_state=101
)
model_tfidf.fit(X_train_tfidf, y_train)

# Save both model and vectorizer
joblib.dump(model_tfidf, 'logreg_tfidf_model.joblib')
joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')

"""## Random Forest

#### *Implementation*
"""

from sklearn.ensemble import RandomForestClassifier

# Model for original and weighted training
model_base = RandomForestClassifier(n_estimators=500, random_state=42, class_weight='balanced')

# Model for oversampled training (no class_weight)
model_oversampled = RandomForestClassifier(n_estimators=500, random_state=42)

run_experiments(
    base_model=model_base,
    oversampled_model=model_oversampled,
    model_name_base="Random Forest",
    X_train_vectorised=X_train_tfidf,
    X_test_vectorised=X_test_tfidf,
    X_train_combined=X_train_combined,
    X_test_combined=X_test_combined,
    y_train=y_train,
    y_test=y_test,
    class_names=lbl_enc.classes_,
    perform_smote=False
)

"""## SVM

#### *Implementation*
"""

from sklearn.svm import LinearSVC

# Model for original and weighted training
model_base = LinearSVC(class_weight='balanced', random_state=42)


# Model for oversampled training (no class_weight)
model_oversampled = LinearSVC(random_state=42)

run_experiments(
    base_model=model_base,
    oversampled_model=model_oversampled,
    model_name_base="SVM",
    X_train_vectorised=X_train_tfidf,
    X_test_vectorised=X_test_tfidf,
    X_train_combined=X_train_combined,
    X_test_combined=X_test_combined,
    y_train=y_train,
    y_test=y_test,
    class_names=lbl_enc.classes_,
    perform_smote=False
)

"""## Naïve Bayes (ComplementNB - for imbalance)

#### *Implementation*
"""

from sklearn.naive_bayes import ComplementNB

# Model for original and weighted training
model_base = ComplementNB(alpha=0.1)  # We don’t handle the class imbalance explicitly

# Model for oversampled training (no class_weight)
model_oversampled = ComplementNB(alpha=0.1)

run_experiments(
    base_model=model_base,
    oversampled_model=model_oversampled,
    model_name_base="NB",
    X_train_vectorised=X_train_tfidf,
    X_test_vectorised=X_test_tfidf,
    X_train_combined=X_train_combined,
    X_test_combined=X_test_combined,
    y_train=y_train,
    y_test=y_test,
    class_names=lbl_enc.classes_,
    perform_smote=False,
    try_combined=False,
    manual_weights=True
)

"""## XGBoost

#### *Implementation*
"""

from xgboost import XGBClassifier

# Model for original and weighted training
model_base = XGBClassifier(learning_rate=0.2, max_depth=7, n_estimators=500, random_state=101, tree_method = "hist", device = "cuda")

# Model for oversampled training (no class_weight)
model_oversampled = XGBClassifier(learning_rate=0.2, max_depth=7, n_estimators=500, random_state=101, tree_method = "hist", device = "cuda")

run_experiments(
    base_model=model_base,
    oversampled_model=model_oversampled,
    model_name_base="XGBoost",
    X_train_vectorised=X_train_tfidf,
    X_test_vectorised=X_test_tfidf,
    X_train_combined=X_train_combined,
    X_test_combined=X_test_combined,
    y_train=y_train,
    y_test=y_test,
    class_names=lbl_enc.classes_,
    manual_weights=True,
    perform_smote=False
)

"""#### *Best COMBINED TF-IDF model*"""

import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.utils.class_weight import compute_class_weight # Import the function

classes = np.unique(y_train)
class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)
weight_dict = {cls: weight for cls, weight in zip(classes, class_weights)}
sample_weights = np.array([weight_dict[label] for label in y_train])

# Train XGB model
model_tfidf_combined = XGBClassifier(learning_rate=0.2, max_depth=7, n_estimators=500, random_state=101, tree_method = "hist", device = "cuda")
model_tfidf_combined.fit(X_train_combined, y_train, sample_weight=sample_weights)
# Save both model and vectorizer
joblib.dump(model_tfidf_combined, 'XGB_tfidf_comb_model.joblib')
joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')

"""# **Word2Vec**

### Vectorising & Combining

To identify the best-performing classification models for our mental health-related text dataset, we followed a structured machine learning pipeline:

1. **Preprocessing and Embedding**
  
  The dataset was already cleaned and tokenized. A custom Word2Vec model was trained on the tokenized text using the gensim library with a vector size of 100. Each document was then converted to a fixed-size embedding by averaging its word vectors.

2. **Train-Test Split**

  The dataset was split into training and test sets using stratified sampling to preserve class distribution.

3. **Model Training with Hyperparameter Tuning**
  
  We trained and evaluated the following models using GridSearchCV with 5-fold cross-validation:
* Random Forest

* Support Vector Machine (SVM)

* Naïve Bayes (GaussianNB)

* Extreme gradient boosting (XGBoost)

  For each model, a set of relevant hyperparameters was explored to identify the configuration that maximized accuracy on the training data.

4. **Evaluation**
  
  The best model from each search was evaluated on the test set. We collected performance metrics including:

* Accuracy

* F1 Score

* Confusion Matrix

* Classification Report
"""

!pip install --force-reinstall numpy==1.24.4
!pip install --upgrade gensim

import numpy as np
import gensim
from gensim.models import Word2Vec
from tqdm import tqdm

# Upload the model
from gensim.models import Word2Vec
w2v_model = Word2Vec.load("w2v_vectorizer.bin")

# Function to average word vectors for a document
def document_vector(doc, w2v_model):
    # Keep only words that exist in the model
    doc = [word for word in doc if word in w2v_model.wv]
    if len(doc) == 0:
        return np.zeros(w2v_model.vector_size)
    return np.mean(w2v_model.wv[doc], axis=0)

X_train_w2v = np.array([document_vector(tokens, w2v_model) for tokens in X_train['tokens']])
X_test_w2v = np.array([document_vector(tokens, w2v_model) for tokens in X_test['tokens']])

# 2. Combine with 'word_count'
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_wc = scaler.fit_transform(X_train[['word_count']])
X_test_wc = scaler.transform(X_test[['word_count']])

X_train_combined = np.hstack([X_train_w2v, X_train_wc])
X_test_combined = np.hstack([X_test_w2v, X_test_wc])

"""## Logistic Regression

#### *Implementations*
"""

from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier

# Model for original and weighted training
model_base = LogisticRegression(solver='lbfgs', penalty='l2', C=10, class_weight='balanced', max_iter=1000, random_state=101)

# Model for oversampled training (no class_weight)
model_oversampled = LogisticRegression(solver='lbfgs', penalty='l2', C=10, max_iter=1000, random_state=101)

run_experiments(
    base_model=model_base,
    oversampled_model=model_oversampled,
    model_name_base="LogReg",
    X_train_vectorised=X_train_w2v,
    X_test_vectorised=X_test_w2v,
    X_train_combined=X_train_combined,
    X_test_combined=X_test_combined,
    y_train=y_train,
    y_test=y_test,
    class_names=lbl_enc.classes_
)

# We do not manually compute class weights because of the param: class_weight='balanced'

"""## Random Forest

### *Implementations*
"""

from sklearn.ensemble import RandomForestClassifier

# Model for original and weighted training
model_base = RandomForestClassifier(n_estimators=500, random_state=42, class_weight='balanced')

# Model for oversampled training (no class_weight)
model_oversampled = RandomForestClassifier(n_estimators=500, random_state=42)

run_experiments(
    base_model=model_base,
    oversampled_model=model_oversampled,
    model_name_base="Random Forest",
    X_train_vectorised=X_train_w2v,
    X_test_vectorised=X_test_w2v,
    X_train_combined=X_train_combined,
    X_test_combined=X_test_combined,
    y_train=y_train,
    y_test=y_test,
    class_names=lbl_enc.classes_
)

"""## SVM

### *Implementations*
"""

from sklearn.svm import LinearSVC

# Model for original and weighted training
model_base = LinearSVC(class_weight='balanced', random_state=42)


# Model for oversampled training (no class_weight)
model_oversampled = LinearSVC(random_state=42)

run_experiments(
    base_model=model_base,
    oversampled_model=model_oversampled,
    model_name_base="SVM",
    X_train_vectorised=X_train_w2v,
    X_test_vectorised=X_test_w2v,
    X_train_combined=X_train_combined,
    X_test_combined=X_test_combined,
    y_train=y_train,
    y_test=y_test,
    class_names=lbl_enc.classes_
)

"""## Naïve Bayes

For dense inputs (Word2Vec/BERT/MISTRAL), NB is not ideal. We skip it.

## XGBoost

### *Implementations*
"""

from xgboost import XGBClassifier

# Model for original and weighted training
model_base = XGBClassifier(learning_rate=0.2, max_depth=7, n_estimators=500, random_state=101, tree_method = "hist", device = "cuda")

# Model for oversampled training (no class_weight)
model_oversampled = XGBClassifier(learning_rate=0.2, max_depth=7, n_estimators=500, random_state=101, tree_method = "hist", device = "cuda")

run_experiments(
    base_model=model_base,
    oversampled_model=model_oversampled,
    model_name_base="XGBoost",
    X_train_vectorised=X_train_w2v,
    X_test_vectorised=X_test_w2v,
    X_train_combined=X_train_combined,
    X_test_combined=X_test_combined,
    y_train=y_train,
    y_test=y_test,
    class_names=lbl_enc.classes_,
    manual_weights=True
)

"""### *Best W2V only model*"""

import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.utils.class_weight import compute_class_weight # Import the function

classes = np.unique(y_train)
class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)
weight_dict = {cls: weight for cls, weight in zip(classes, class_weights)}
sample_weights = np.array([weight_dict[label] for label in y_train])

import joblib
from xgboost import XGBClassifier

# Train XGB model
model_w2v = XGBClassifier(learning_rate=0.2, max_depth=7, n_estimators=500, random_state=101, tree_method = "hist", device = "cuda")

model_w2v.fit(X_train_w2v, y_train)

# Save both model and vectorizer
joblib.dump(model_w2v, 'XGB_w2v_model.joblib')

"""### *Best COMBINED W2V model*"""

import joblib
from imblearn.over_sampling import RandomOverSampler, SMOTE

model_w2v_combined = XGBClassifier(learning_rate=0.2, max_depth=7, n_estimators=500, random_state=101, tree_method = "hist", device = "cuda")

smote = SMOTE(random_state=101)
X_train_smote, y_train_smote = smote.fit_resample(X_train_combined, y_train)

# Train RF model
model_w2v_combined.fit(X_train_smote, y_train_smote)

# Save both model and vectorizer
joblib.dump(model_w2v_combined, 'xgb_w2v_combined_model.joblib')

"""# **BERT**

#### *BERT Encoding*
"""

!pip install --upgrade --force-reinstall numpy

!pip uninstall -y transformers sentence-transformers
!pip install -U --no-cache-dir transformers==4.40.1 sentence-transformers==2.6.1

df = pd.read_csv("df_PrePreprocessing.csv")

import pandas as pd
import numpy as np
import re
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sentence_transformers import SentenceTransformer
from google.colab import files
import pickle

# -----------------------------
# 1. Load + clean the text
# -----------------------------
def clean_for_bert(text):
    text = re.sub(r'<.*?>+', '', text)              # Remove HTML tags
    text = re.sub(r'http[s]?://\S+', '', text)      # Remove URLs
    text = re.sub(r'\n', ' ', text)                 # Replace newlines with space
    text = re.sub(r'\s+', ' ', text).strip()        # Normalize spacing
    return text

df['cleaned_for_bert'] = df['statement'].astype(str).apply(clean_for_bert)

# -----------------------------
# 2. Encode labels
# -----------------------------
lbl_enc = LabelEncoder()
df['label_encoded'] = lbl_enc.fit_transform(df['status'])

# -----------------------------
# 3. Generate BERT embeddings
# -----------------------------
# The SentenceTransformer.encode() method handles tokenization, padding, truncation, and batching internally.
bert_model = SentenceTransformer('all-MiniLM-L6-v2')
df['bert_embedding'] = [bert_model.encode(text) for text in tqdm(df['cleaned_for_bert'])]

# -----------------------------
# 4. Save final dataframe
# -----------------------------
pickle_path = "df_BERT_embeddings.pkl"

df.to_pickle(pickle_path)

files.download(pickle_path)

"""#### *BERT Encoding*"""

!pip install --upgrade --force-reinstall numpy

!pip uninstall -y transformers sentence-transformers
!pip install -U --no-cache-dir transformers==4.40.1 sentence-transformers==2.6.1

import pandas as pd
import numpy as np
import re
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sentence_transformers import SentenceTransformer
from google.colab import files
import pickle

# -----------------------------
# 1. Load + clean the text
# -----------------------------
def clean_for_bert(text):
    text = re.sub(r'<.*?>+', '', text)              # Remove HTML tags
    text = re.sub(r'http[s]?://\S+', '', text)      # Remove URLs
    text = re.sub(r'\n', ' ', text)                 # Replace newlines with space
    text = re.sub(r'\s+', ' ', text).strip()        # Normalize spacing
    return text

df['cleaned_for_bert'] = df['statement'].astype(str).apply(clean_for_bert)

# -----------------------------
# 2. Encode labels
# -----------------------------
lbl_enc = LabelEncoder()
df['label_encoded'] = lbl_enc.fit_transform(df['status'])

# -----------------------------
# 3. Generate BERT embeddings
# -----------------------------
# The SentenceTransformer.encode() method handles tokenization, padding, truncation, and batching internally.
bert_model = SentenceTransformer('all-MiniLM-L6-v2')
df['bert_embedding'] = [bert_model.encode(text) for text in tqdm(df['cleaned_for_bert'])]

# -----------------------------
# 4. Save final dataframe
# -----------------------------
pickle_path = "df_BERT_embeddings.pkl"

df.to_pickle(pickle_path)

files.download(pickle_path)

"""#### *Splitting and combining*:"""

import pandas as pd
df = pd.read_pickle("df_BERT_embeddings.pkl")

from sklearn.preprocessing import LabelEncoder, StandardScaler

# -----------------------------
# 1. Combine BERT + word_count
# -----------------------------
X = df[['bert_embedding', 'word_count']]
y = df['label_encoded'].values

# -----------------------------
# 2. Train/Test Split
# -----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=101, stratify=y
)

# -----------------------------
# 3. Separate BERT and word_count
# -----------------------------
X_train_bert = np.vstack(X_train['bert_embedding'])
X_test_bert = np.vstack(X_test['bert_embedding'])

scaler = StandardScaler()
X_train_wc = scaler.fit_transform(X_train[['word_count']])
X_test_wc = scaler.transform(X_test[['word_count']])  # Only transform here!

# -----------------------------
# 4. Combine
# -----------------------------
X_train_combined = np.hstack([X_train_bert, X_train_wc])
X_test_combined = np.hstack([X_test_bert, X_test_wc])

"""## Logistic Regression

#### *Implementations*
"""

from sklearn.linear_model import LogisticRegression

# Model for original and weighted training
model_base = LogisticRegression(solver='lbfgs', penalty='l2', C=10, class_weight='balanced', max_iter=1000, random_state=101)

# Model for oversampled training (no class_weight)
model_oversampled = LogisticRegression(solver='lbfgs', penalty='l2', C=10, max_iter=1000, random_state=101)

run_experiments(
    base_model=model_base,
    oversampled_model=model_oversampled,
    model_name_base="LogReg",
    X_train_vectorised=X_train_bert,
    X_test_vectorised=X_test_bert,
    X_train_combined=X_train_combined,
    X_test_combined=X_test_combined,
    y_train=y_train,
    y_test=y_test,
    class_names=lbl_enc.classes_
)

# We do not manually compute class weights because of the param: class_weight='balanced'

"""## Random Forest

### *Implementations*
"""

from sklearn.ensemble import RandomForestClassifier

# Model for original and weighted training
model_base = RandomForestClassifier(n_estimators=500, random_state=42, class_weight='balanced')

# Model for oversampled training (no class_weight)
model_oversampled = RandomForestClassifier(n_estimators=500, random_state=42)

run_experiments(
    base_model=model_base,
    oversampled_model=model_oversampled,
    model_name_base="Random Forest",
    X_train_vectorised=X_train_bert,
    X_test_vectorised=X_test_bert,
    X_train_combined=X_train_combined,
    X_test_combined=X_test_combined,
    y_train=y_train,
    y_test=y_test,
    class_names=lbl_enc.classes_
)

"""## SVM

### *Implementations*
"""

from sklearn.svm import LinearSVC

# Model for original and weighted training
model_base = LinearSVC(class_weight='balanced', random_state=42)


# Model for oversampled training (no class_weight)
model_oversampled = LinearSVC(random_state=42)

run_experiments(
    base_model=model_base,
    oversampled_model=model_oversampled,
    model_name_base="SVM",
    X_train_vectorised=X_train_bert,
    X_test_vectorised=X_test_bert,
    X_train_combined=X_train_combined,
    X_test_combined=X_test_combined,
    y_train=y_train,
    y_test=y_test,
    class_names=lbl_enc.classes_
)

"""## Naïve Bayes

For dense inputs (Word2Vec/BERT/MISTRAL), NB is not ideal. We skip it.

## XGBoost

### *Implementations*
"""

from xgboost import XGBClassifier

# Model for original and weighted training
model_base = XGBClassifier(learning_rate=0.2, max_depth=7, n_estimators=500, random_state=101, tree_method = "hist", device = "cuda")

# Model for oversampled training (no class_weight)
model_oversampled = XGBClassifier(learning_rate=0.2, max_depth=7, n_estimators=500, random_state=101, tree_method = "hist", device = "cuda")

run_experiments(
    base_model=model_base,
    oversampled_model=model_oversampled,
    model_name_base="XGBoost",
    X_train_vectorised=X_train_bert,
    X_test_vectorised=X_test_bert,
    X_train_combined=X_train_combined,
    X_test_combined=X_test_combined,
    y_train=y_train,
    y_test=y_test,
    class_names=lbl_enc.classes_,
    manual_weights=True
)

"""### *Best BERT only model*"""

import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.utils.class_weight import compute_class_weight # Import the function

classes = np.unique(y_train)
class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)
weight_dict = {cls: weight for cls, weight in zip(classes, class_weights)}
sample_weights = np.array([weight_dict[label] for label in y_train])

# Train XGB model
model_bert = XGBClassifier(learning_rate=0.2, max_depth=7, n_estimators=500, random_state=101, tree_method = "hist", device = "cuda")
model_bert.fit(X_train_bert, y_train, sample_weight=sample_weights)
# Save both model and vectorizer
joblib.dump(model_bert, 'XGB_bert_model.joblib')

"""### *Best COMBINED BERT model*"""

import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import RandomOverSampler

model_bert_combined = XGBClassifier(learning_rate=0.2, max_depth=7, n_estimators=500, random_state=101, tree_method = "hist", device = "cuda")

ros = RandomOverSampler(random_state=101)
X_train_ros, y_train_ros = ros.fit_resample(X_train_combined, y_train)

# Train XGB model
model_bert_combined.fit(X_train_ros, y_train_ros)

# Save both model and vectorizer
joblib.dump(model_bert_combined, 'XGB_bert_combined_model.joblib')

"""# **MISTRAL**

#### *MISTRAL Encoding*
"""

from huggingface_hub import login

login()

import pandas as pd
import re
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm
from sklearn.preprocessing import LabelEncoder
from google.colab import files


# -----------------------------
# 1. Load + clean the text (same as before)
# -----------------------------
def clean_for_mistral(text):
    text = re.sub(r'<.*?>+', '', text)              # Remove HTML tags
    text = re.sub(r'http[s]?://\S+', '', text)      # Remove URLs
    text = re.sub(r'\n', ' ', text)                 # Replace newlines with space
    text = re.sub(r'\s+', ' ', text).strip()        # Normalize spacing
    return text

df = pd.read_csv("df_PrePreprocessing.csv")
df['cleaned_for_mistral'] = df['statement'].astype(str).apply(clean_for_mistral)

# -----------------------------
# 2. Encode labels (as before)
# -----------------------------
lbl_enc = LabelEncoder()
df['label_encoded'] = lbl_enc.fit_transform(df['status'])

# -----------------------------
# 3. Load Mistral model and tokenizer
# -----------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name = "mistralai/Mistral-7B-Instruct-v0.1"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)
model.to(device)
model.eval()

# -----------------------------
# 4. Function to extract mean-pooled Mistral embeddings
# -----------------------------
def get_mistral_embedding(text):
    with torch.no_grad():
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        outputs = model(**inputs)
        hidden_states = outputs.last_hidden_state.squeeze(0)
        embedding = hidden_states.mean(dim=0).cpu().numpy().astype(float).tolist()
    return embedding

# -----------------------------
# 5. Generate embeddings for each post
# -----------------------------
df['mistral_embedding'] = [get_mistral_embedding(text) for text in tqdm(df['cleaned_for_mistral'])]

# -----------------------------
# 6. Save final dataframe
# -----------------------------
pickle_path = "df_MISTRAL_embeddings.pkl"

df.to_pickle(pickle_path)

files.download(pickle_path)

"""#### *Splitting and combining*:"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder

import pickle

with open("df_MISTRAL_embeddings.pkl", "rb") as f:
    df = pickle.load(f)

# Ensure embeddings are in the correct format (shape: [n_samples, embedding_dim])
mistral_vectors = np.vstack(df['mistral_embedding'].values)

# -----------------------------
# 2. Encode labels
# -----------------------------
lbl_enc = LabelEncoder()
df['label_encoded'] = lbl_enc.fit_transform(df['status'])

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split

# -----------------------------
# 1. Combine MISTRAL + word_count
# -----------------------------
X = df[['mistral_embedding', 'word_count']]
y = df['label_encoded'].values

# -----------------------------
# 2. Train/Test Split
# -----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=101, stratify=y
)

# -----------------------------
# 3. Separate BERT and word_count
# -----------------------------
X_train_mistral = np.vstack(X_train['mistral_embedding'])
X_test_mistral = np.vstack(X_test['mistral_embedding'])

scaler = StandardScaler()
X_train_wc = scaler.fit_transform(X_train[['word_count']])
X_test_wc = scaler.transform(X_test[['word_count']])  # Only transform here!

# -----------------------------
# 4. Combine
# -----------------------------
X_train_combined = np.hstack([X_train_mistral, X_train_wc])
X_test_combined = np.hstack([X_test_mistral, X_test_wc])

"""## Logistic Regression

#### *Implementations*
"""

from sklearn.linear_model import LogisticRegression

# Model for original and weighted training
model_base = LogisticRegression(solver='lbfgs', penalty='l2', C=10, class_weight='balanced', max_iter=2000, random_state=101)

# Model for oversampled training (no class_weight)
model_oversampled = LogisticRegression(solver='lbfgs', penalty='l2', C=10, max_iter=2000, random_state=101)

run_experiments(
    base_model=model_base,
    oversampled_model=model_oversampled,
    model_name_base="LogReg",
    X_train_vectorised=X_train_mistral,
    X_test_vectorised=X_test_mistral,
    X_train_combined=X_train_combined,
    X_test_combined=X_test_combined,
    y_train=y_train,
    y_test=y_test,
    class_names=lbl_enc.classes_
)

# We do not manually compute class weights because of the param: class_weight='balanced'

"""## Random Forest

### *Implementations*
"""

from sklearn.ensemble import RandomForestClassifier

# Model for original and weighted training
model_base = RandomForestClassifier(n_estimators=500, random_state=42, class_weight='balanced')

# Model for oversampled training (no class_weight)
model_oversampled = RandomForestClassifier(n_estimators=500, random_state=42)

run_experiments(
    base_model=model_base,
    oversampled_model=model_oversampled,
    model_name_base="Random Forest",
    X_train_vectorised=X_train_mistral,
    X_test_vectorised=X_test_mistral,
    X_train_combined=X_train_combined,
    X_test_combined=X_test_combined,
    y_train=y_train,
    y_test=y_test,
    class_names=lbl_enc.classes_
)

"""## SVM

### *Implementations*
"""

from sklearn.svm import LinearSVC

# Model for original and weighted training
model_base = LinearSVC(class_weight='balanced', random_state=42)


# Model for oversampled training (no class_weight)
model_oversampled = LinearSVC(random_state=42)

run_experiments(
    base_model=model_base,
    oversampled_model=model_oversampled,
    model_name_base="SVM",
    X_train_vectorised=X_train_mistral,
    X_test_vectorised=X_test_mistral,
    X_train_combined=X_train_combined,
    X_test_combined=X_test_combined,
    y_train=y_train,
    y_test=y_test,
    class_names=lbl_enc.classes_
)

"""## Naïve Bayes

For dense inputs (Word2Vec/BERT/MISTRAL), NB is not ideal. We skip it.

## XGBoost

### *Implementations*
"""

from xgboost import XGBClassifier

# Model for original and weighted training
model_base = XGBClassifier(learning_rate=0.2, max_depth=7, n_estimators=500, random_state=101, tree_method = "hist", device = "cuda")

# Model for oversampled training (no class_weight)
model_oversampled = XGBClassifier(learning_rate=0.2, max_depth=7, n_estimators=500, random_state=101, tree_method = "hist", device = "cuda")

run_experiments(
    base_model=model_base,
    oversampled_model=model_oversampled,
    model_name_base="XGBoost",
    X_train_vectorised=X_train_mistral,
    X_test_vectorised=X_test_mistral,
    X_train_combined=X_train_combined,
    X_test_combined=X_test_combined,
    y_train=y_train,
    y_test=y_test,
    class_names=lbl_enc.classes_,
    manual_weights=True
)

"""### *Best MISTRAL only model*"""

import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.utils.class_weight import compute_class_weight # Import the function

classes = np.unique(y_train)
class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)
weight_dict = {cls: weight for cls, weight in zip(classes, class_weights)}
sample_weights = np.array([weight_dict[label] for label in y_train])

# Train XGB model
model_mistral = XGBClassifier(learning_rate=0.2, max_depth=7, n_estimators=500, random_state=101, tree_method = "hist", device = "cuda")
model_mistral.fit(X_train_mistral, y_train, sample_weight=sample_weights)
# Save both model and vectorizer
joblib.dump(model_mistral, 'XGB_mistral_model.joblib')

"""### *Best COMBINED MISTRAL model*"""

import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import RandomOverSampler

model_mistral_combined = XGBClassifier(learning_rate=0.2, max_depth=7, n_estimators=500, random_state=101, tree_method = "hist", device = "cuda")

ros = RandomOverSampler(random_state=101)
X_train_ros, y_train_ros = ros.fit_resample(X_train_combined, y_train)

# Train XGB model
model_mistral_combined.fit(X_train_ros, y_train_ros)

# Save both model and vectorizer
joblib.dump(model_mistral_combined, 'XGB_mistral_combined_model.joblib')

"""# **Comments & Embedding Visualisations**

## **Notes**

### *TF-IDF better than W2V*

Why TF-IDF could be performing Better Than Word2Vec:

1. **TF-IDF captures class-specific vocabulary patterns**
- It emphasizes distinctive terms (e.g., “depression”, “bipolar”, “panic”) that might be highly class-indicative.
- Even rare words get higher weights if they help distinguish classes.
- Great for text classification when:
  - Vocabulary is rich and discriminative
  - Class definitions are based on specific terms or phrases

2. **Word2Vec averages lose contextual nuance**
- You're averaging all word vectors in a post and this blurs specific semantic signals.
- You lose:

  - Word order
  - Negation cues ("not suicidal" ≈ "suicidal")
  - Important rare or unique terms
  - Word2Vec is better for:
  - Semantic similarity
  - Retrieval
  - Clustering
  - But not always for fine-grained classification

3. **Word2Vec requires large corpora to shine**
- Trained on your dataset → maybe not large/diverse enough to capture deep semantic differences
- Even pre-trained vectors (like GloVe) may perform worse if class cues are highly lexical

### *What have we done*

In this project, we set up a complete and well-organized system to evaluate machine learning models on an imbalanced dataset with both text and numeric features. The idea was to test different ways of turning text into something models can understand (vectorization), and then compare how various machine learning models perform—especially in terms of how fairly they treat each class, since the dataset is imbalanced.

#### **Feature Representation: Text Vectorization**

We used four different techniques to convert raw text (in this case, user statements) into numerical features:

* TF-IDF – A classic method that works well with linear models like logistic regression or SVM. It treats text like a bag of words and assigns weights based on how rare or common a word is.

* Word2Vec – This captures meaning better by representing each word as a vector and then averaging them for a sentence.

* BERT – A transformer-based model that gives contextual embeddings, meaning it understands a word based on its surroundings. We used mean-pooled BERT embeddings per statement.

* Mistral – A large language model similar to BERT but more modern. Again, we used mean-pooled embeddings to get a fixed-length vector for each statement.

On top of these, we optionally added a numerical feature: the standardized word count of each statement. This small addition sometimes helped the models by giving a sense of length or verbosity.

#### **The Models We Trained**

We picked a set of commonly used classifiers, each with known performance characteristics and good benchmark support:

* Logistic Regression: A strong baseline, especially with regularization. We mainly used solver='lbfgs', penalty='l2', and class_weight='balanced' to deal with imbalance.

* Random Forest: A tree-based ensemble that's robust and can handle mixed feature types. We tested both balanced and default settings.

* Support Vector Machine (SVM): Known for high accuracy, especially on text data. We used a linear kernel to keep it fast and interpretable.

* Naive Bayes: We used BernoulliNB and ComplementNB, mostly with TF-IDF, since it's less effective on dense embeddings like Word2Vec or BERT.

* XGBoost: A powerful gradient boosting model, set up with GPU support (tree_method='gpu_hist'), and tuned based on settings found in previous studies.

Each model was configured with fixed hyperparameters found in past benchmark work, ***so we didn't use grid search*** (to save time and keep comparisons fair and fast).

#### **Class Imbalance: How We Handled It**

Our dataset was clearly imbalanced — with many more "Normal" and "Depression" examples than "Bipolar" or "Anxiety." We tackled this issue in multiple ways depending on the setup:

* Class weights: For models that support it (like Logistic Regression or Random Forest), we used class_weight='balanced'. For others (XGBoost), we calculated manual weights using compute_class_weight().

* Resampling: For other models or to go further, we applied:
  * Random Oversampling (ROS): This duplicates examples from minority classes.
  * SMOTE: This generates synthetic examples, but we avoided it with TF-IDF due to the nature of sparse matrices.

We made sure to not mix class weighting with oversampling, since that would be redundant or even harmful.

#### **How It All Came Together**

To manage all these combinations, we built a custom function: run_experiments(). It lets us easily test any model across multiple setups, like:
* Text only
* Text + numeric
* With oversampling (ROS or SMOTE)

This function outputs all the key metrics we care about — accuracy, F1 macro (average across all classes), and F1 micro (overall correct predictions). It also generates visualizations like:
* Bar plots to compare model performance
* Confusion matrices to understand which classes are doing well or poorly

## **t-SNE Embedding Visualisations**

Let's visualize the semantic space of Reddit posts by reducing high-dimensional BERT embeddings into 2D space. This helps us see how different mental health categories cluster or overlap.

Embeddings are hundreds of dimensions (e.g., 384 for MiniLM).t-SNE and UMAP reduce this to 2D so we can plot and:
- Visually explore semantic patterns in the data.
- See if different mental health categories cluster together or overlap.
- Validate how well our embeddings capture category-level meaning.

### **t-SNE vs PCA vs UMAP**

Initially, we chose t-SNE over PCA and UMAP because our goal was to visually explore whether mental health posts with similar emotional meaning cluster together in embedding space. t-SNE is especially good at preserving local structure, which helps reveal small, meaningful groupings. This is ideal for analyzing subtle differences between posts, particularly with BERT and Word2Vec embeddings.

While PCA is fast and preserves global variance, it struggles with nonlinear patterns common in language data. UMAP is a strong alternative that preserves both local and global structure and scales well, but t-SNE tends to produce clearer visual clusters, which made it more effective for our qualitative analysis.

Still, we can also try to visualise the text vectorisers using PCA or UMAP:
"""

# PCA
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_train_w2v)

plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_train, cmap='tab10', alpha=0.7)
plt.title("PCA of Word2Vec Document Embeddings")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.colorbar()
plt.show()

# UMAP
# ...

"""## **TF-IDF**

### *Without numeric combination*

**Version 01**
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# Vectorize text
vectorizer = TfidfVectorizer(max_features=5000)
X_tfidf = vectorizer.fit_transform(df['tokens'])

# Reduce dimensions with t-SNE
X_2d = TSNE(n_components=2, random_state=42).fit_transform(X_tfidf.toarray())

# Plot
plt.figure(figsize=(10, 6))
plt.scatter(X_2d[:, 0], X_2d[:, 1], c=df['status'].astype('category').cat.codes, cmap='tab10', alpha=0.7)
plt.title("TF-IDF Vectors Projected to 2D via t-SNE")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.colorbar(label='Mental Health Category')
plt.show()

"""**Version 02**"""

# Plot
df_vis = pd.DataFrame(X_2d, columns=['x', 'y'])
df_vis['label'] = df['status'].values

import seaborn as sns

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_vis, x='x', y='y', hue='label', palette='tab10', alpha=0.7)
plt.title("TF-IDF Vectors Projected to 2D via t-SNE")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.legend(title="Mental Health Category")
plt.grid(True)
plt.show()

"""### *With numeric combination*

#### *Not standardized*
"""

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from scipy.sparse import hstack
from sklearn.preprocessing import LabelEncoder

# Combine TF-IDF and numeric feature
X_combined = hstack([X_tfidf, X[['word_count']].values])  # X_num should be shape (n_samples, 1)

# Convert sparse to dense for t-SNE
X_dense = X_combined.toarray()

# Apply t-SNE
X_2d_tfidf_comb = TSNE(n_components=2, random_state=42).fit_transform(X_dense)

# Prepare label data
le = LabelEncoder()
y_encoded = le.fit_transform(y)  # y is the category column, e.g., df['status']

# Plot
df_vis = pd.DataFrame(X_2d_tfidf_comb, columns=['Component 1', 'Component 2'])
df_vis['Category'] = y_encoded

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_vis, x='Component 1', y='Component 2', hue='Category', palette='tab10', alpha=0.7)
plt.title("t-SNE Projection of TF-IDF + word_count (Full Dataset)")
plt.grid(True)
plt.legend(title='Mental Health Category', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

"""#### *Standardized*

**Version 01**
"""

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from scipy.sparse import hstack
from sklearn.preprocessing import LabelEncoder

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_wordcount_scaled = scaler.fit_transform(X[['word_count']])

X_combined = hstack([X_tfidf, X_wordcount_scaled])


# Convert sparse to dense for t-SNE
X_dense = X_combined.toarray()

# Apply t-SNE
X_2d_tfidf_comb_std = TSNE(n_components=2, random_state=42).fit_transform(X_dense)

# Encode labels for visualization
le = LabelEncoder()
labels = le.fit_transform(df['status'])

# Plot
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_2d_tfidf_comb_std[:, 0], X_2d_tfidf_comb_std[:, 1], c=labels, cmap='tab10', alpha=0.7)
plt.title("TF-IDF + Standardized word_count Vectors Projected to 2D via t-SNE")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.colorbar(scatter, label='Mental Health Category')
plt.grid(True)
plt.show()

"""**Version 02**"""

# Plot
df_vis = pd.DataFrame(X_2d_tfidf_comb_std, columns=['x', 'y'])
df_vis['label'] = df['status'].values

import seaborn as sns

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_vis, x='x', y='y', hue='label', palette='tab10', alpha=0.7)
plt.title("TF-IDF + Standardized word_count Vectors Projected to 2D via t-SNE")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.legend(title="Mental Health Category")
plt.grid(True)
plt.show()

"""To investigate whether post length (word count) contributes meaningful information to the vector space, we repeated our t-SNE projections after concatenating the word_count (standardized) to each embedding. The impact of this addition varies significantly across methods.

The effect is most visible in the TF-IDF projection. The original TF-IDF plot showed a dense central region, with all categories overlapping heavily, especially “Normal,” which dominated the center. After adding word_count, we observe:
-  more spread-out structure, with some clearer separations between emotional categories.
- “Normal” documents shift toward a separate arc-like region, suggesting that their typically shorter length helped distinguish them from longer, more emotionally loaded posts.

This makes sense because TF-IDF does not capture semantic structure or sentence context, it's only based on word frequency. So, adding a numerical cue like length helps introduce external structure, breaking ties between similarly weighted vectors.

## **W2V**

#### Pre-w2v
"""

!pip install --force-reinstall numpy==1.24.4
!pip install --upgrade gensim

import numpy as np
import gensim
from gensim.models import Word2Vec
from tqdm import tqdm

# Upload the model
from gensim.models import Word2Vec
w2v_model = Word2Vec.load("w2v_model.bin")

df = pd.read_csv("df_PostPreprocessing_5classes.csv")

from sklearn.preprocessing import LabelEncoder
# Splitting the data
X = df[['tokens','word_count']]
y = df['status']

lbl_enc = LabelEncoder()
y = lbl_enc.fit_transform(y.values)

# Train Word2Vec on the tokenized column
# w2v_model = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=2, workers=4)

# Function to get average Word2Vec vector for a document
def document_vector(tokens):
    vectors = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]
    return np.mean(vectors, axis=0) if vectors else np.zeros(w2v_model.vector_size)

# Compute document vectors
doc_vectors = np.array([document_vector(tokens) for tokens in df['tokens']])

"""### *Without numeric combination*

**Version 01**
"""

import numpy as np
import matplotlib.pyplot as plt
from gensim.models import Word2Vec
from sklearn.manifold import TSNE
from sklearn.preprocessing import LabelEncoder

# Apply t-SNE for 2D projection
X_2d_w2v = TSNE(n_components=2, random_state=42).fit_transform(doc_vectors)

# Encode labels for coloring
le = LabelEncoder()
labels = le.fit_transform(df['status'])

# Plot
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_2d_w2v[:, 0], X_2d_w2v[:, 1], c=labels, cmap='tab10', alpha=0.7)
plt.title("t-SNE Projection of Word2Vec Document Embeddings")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.colorbar(scatter, label='Mental Health Category')
plt.grid(True)
plt.show()

"""**Version 02**"""

# Plot
df_vis = pd.DataFrame(X_2d_w2v, columns=['x', 'y'])
df_vis['label'] = df['status'].values

import seaborn as sns

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_vis, x='x', y='y', hue='label', palette='tab10', alpha=0.7)
plt.title("t-SNE of Word2Vec")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.legend(title="Category")
plt.grid(True)
plt.show()

"""### *With numeric combination*

#### *Not standardized*
"""

from sklearn.manifold import TSNE
import numpy as np

# Combine
X_w2v_combined = np.hstack([doc_vectors, X[['word_count']].values])

# t-SNE
X_2d_w2v_comb = TSNE(n_components=2, random_state=42).fit_transform(X_w2v_combined)

# Plot
df_vis = pd.DataFrame(X_2d_w2v_comb, columns=['x', 'y'])
df_vis['label'] = df['status'].values

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_vis, x='x', y='y', hue='label', palette='tab10', alpha=0.7)
plt.title("t-SNE of Word2Vec + word_count")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.legend(title="Category")
plt.grid(True)
plt.show()

"""#### *Standardized*"""

from sklearn.manifold import TSNE
import numpy as np
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_wordcount_scaled = scaler.fit_transform(X[['word_count']])

# Combine
doc_vectors = np.array([document_vector(tokens) for tokens in df['tokens']])
X_w2v_combined = np.hstack([doc_vectors, X_wordcount_scaled])

# t-SNE
X_2d_w2v_comb_std = TSNE(n_components=2, random_state=42).fit_transform(X_w2v_combined)

"""**Version 01**"""

# Encode labels for visualization
le = LabelEncoder()
labels = le.fit_transform(df['status'])

# Plot
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_2d_w2v_comb_std[:, 0], X_2d_w2v_comb_std[:, 1], c=labels, cmap='tab10', alpha=0.7)
plt.title("t-SNE of Word2Vec + word_count")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.colorbar(scatter, label='Mental Health Category')
plt.grid(True)
plt.show()

"""**Version 02**"""

# Plot
df_vis = pd.DataFrame(X_2d_w2v_comb_std, columns=['x', 'y'])
df_vis['label'] = df['status'].values

import seaborn as sns

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_vis, x='x', y='y', hue='label', palette='tab10', alpha=0.7)
plt.title("t-SNE of Word2Vec + word_count")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.legend(title="Category")
plt.grid(True)
plt.show()

"""Word2Vec embeddings already encode semantic relationships, so the effect of adding word_count is less dramatic, but still noticeable. The new projection shows:
- A more curved or layered layout, where “Normal” documents again begin to separate more visibly.

## **BERT**

#### *Pre-BERT*
"""

!pip uninstall -y numpy
!pip install numpy --upgrade --force-reinstall

import pandas as pd
df = pd.read_pickle("df_BERT_embeddings.pkl")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.preprocessing import LabelEncoder

# Ensure embeddings are in the correct format (shape: [n_samples, embedding_dim])
bert_vectors = np.vstack(df['bert_embedding'].values)

"""### *Without numeric combination*

**Version 01**
"""

# Reduce dimensionality with t-SNE
X_2d_bert = TSNE(n_components=2, random_state=42).fit_transform(bert_vectors)

# Encode labels (for coloring)
le = LabelEncoder()
labels = le.fit_transform(df['status'])

# Plot
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_2d_bert[:, 0], X_2d_bert[:, 1], c=labels, cmap='tab10', alpha=0.7)
plt.title("t-SNE Projection of BERT Embeddings")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.colorbar(scatter, label='Mental Health Category')
plt.grid(True)
plt.show()

"""**Version 02**"""

# Plot
df_vis = pd.DataFrame(X_2d_bert, columns=['x', 'y'])
df_vis['label'] = df['status'].values

import seaborn as sns

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_vis, x='x', y='y', hue='label', palette='tab10', alpha=0.7)
plt.title("t-SNE Projection of BERT Embeddings")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.legend(title="Mental Health Category")
plt.grid(True)
plt.show()

"""### *With numeric combination*

#### *Standardized*
"""

X = df[['bert_embedding', 'word_count']]
y = df['label_encoded'].values

"""**Version 01**"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler

# Ensure embeddings are in the correct format (shape: [n_samples, embedding_dim])
bert_vectors = np.vstack(df['bert_embedding'].values)

# Standardize word_count
scaler = StandardScaler()
X_wordcount_scaled = scaler.fit_transform(X[['word_count']])


# Combine
X_bert_combined = np.hstack([bert_vectors, X_wordcount_scaled])

# Reduce dimensionality with t-SNE
X_2d_bert_combined_std = TSNE(n_components=2, random_state=42).fit_transform(X_bert_combined)

# Encode labels for visualization
le = LabelEncoder()
labels = le.fit_transform(df['status'])

# Plot
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_2d_bert_combined_std[:, 0], X_2d_bert_combined_std[:, 1], c=labels, cmap='tab10', alpha=0.7)
plt.title("t-SNE Projection of BERT + word_count")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.colorbar(scatter, label='Mental Health Category')
plt.grid(True)
plt.show()

"""**Version 02**"""

# Plot
df_vis = pd.DataFrame(X_2d_bert_combined_std, columns=['x', 'y'])
df_vis['label'] = df['status'].values

import seaborn as sns

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_vis, x='x', y='y', hue='label', palette='tab10', alpha=0.7)
plt.title("t-SNE of Word2Vec + word_count")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.legend(title="Category")
plt.grid(True)
plt.show()

"""## **Mistral Embeddings**

We decided to implement Mistral embeddings to expand our comparison of embedding methods and include a decoder-only transformer model  (GPT, Mistral...), which represents a different architecture from models like BERT (encoder-based contextual embeddings) or Word2Vec (static embeddings). While our current analysis focuses on traditional (TF-IDF, Word2Vec) and contextual (BERT) encoders, Mistral offers a chance to explore how modern large language models capture meaning in mental health-related text. By comparing its embeddings with those from encoder-based models, we aim to understand whether different transformer architectures lead to different clustering behavior or capture unique semantic patterns in emotionally complex language.

#### *Pre-MISTRAL*
"""

import pandas as pd
df = pd.read_pickle("df_MISTRAL_embeddings.pkl")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.preprocessing import LabelEncoder

# Ensure embeddings are in the correct format (shape: [n_samples, embedding_dim])
mistral_vectors = np.vstack(df['mistral_embedding'].values)

"""### *Without numeric combination*

**Version 01**
"""

# Reduce dimensionality with t-SNE
X_2d_mistral = TSNE(n_components=2, random_state=42).fit_transform(mistral_vectors)

# Encode labels (for coloring)
le = LabelEncoder()
labels = le.fit_transform(df['status'])

# Plot
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_2d_mistral[:, 0], X_2d_mistral[:, 1], c=labels, cmap='tab10', alpha=0.7)
plt.title("t-SNE Projection of MISTRAL Embeddings")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.colorbar(scatter, label='Mental Health Category')
plt.grid(True)
plt.show()

"""**Version 02**"""

# Plot
df_vis = pd.DataFrame(X_2d_mistral, columns=['x', 'y'])
df_vis['label'] = df['status'].values

import seaborn as sns

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_vis, x='x', y='y', hue='label', palette='tab10', alpha=0.7)
plt.title("t-SNE Projection of MISTRAL Embeddings")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.legend(title="Mental Health Category")
plt.grid(True)
plt.show()

"""### *With numeric combination*

#### *Standardized*
"""

X = df[['mistral_embedding', 'word_count']]
y = df['label_encoded'].values

"""**Version 01**"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler

# Ensure embeddings are in the correct format (shape: [n_samples, embedding_dim])
mistral_vectors = np.vstack(df['mistral_embedding'].values)

# Standardize word_count
scaler = StandardScaler()
X_wordcount_scaled = scaler.fit_transform(X[['word_count']])


# Combine
X_mistral_combined = np.hstack([mistral_vectors, X_wordcount_scaled])

# Reduce dimensionality with t-SNE
X_2d_mistral_combined_std = TSNE(n_components=2, random_state=42).fit_transform(X_mistral_combined)

# Encode labels for visualization
le = LabelEncoder()
labels = le.fit_transform(df['status'])

# Plot
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_2d_mistral_combined_std[:, 0], X_2d_mistral_combined_std[:, 1], c=labels, cmap='tab10', alpha=0.7)
plt.title("t-SNE Projection of MISTRAL + word_count")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.colorbar(scatter, label='Mental Health Category')
plt.grid(True)
plt.show()

"""**Version 02**"""

# Plot
df_vis = pd.DataFrame(X_2d_mistral_combined_std, columns=['x', 'y'])
df_vis['label'] = df['status'].values

import seaborn as sns

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_vis, x='x', y='y', hue='label', palette='tab10', alpha=0.7)
plt.title("t-SNE of MISTRAL + word_count")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.legend(title="Category")
plt.grid(True)
plt.show()