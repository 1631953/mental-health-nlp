# -*- coding: utf-8 -*-
"""04_daic_eda_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/124ira5eilbqyr4w5nMpJafWs0n6stKNU

#### *Libraries*
"""

import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
import re
import os
import spacy
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import seaborn as sns
from wordcloud import WordCloud

"""# **Dataset construction**

## *Labels (Status)*

First, let's get the final label set for each participant:
"""

import pandas as pd

# Load the 3 label files
train = pd.read_csv('train_split_Depression_AVEC2017.csv')
dev = pd.read_csv('dev_split_Depression_AVEC2017.csv')
test = pd.read_csv('full_test_split.csv')
test = test.rename(columns={'PHQ_Score': 'PHQ8_Score', 'PHQ_Binary': 'PHQ8_Binary'})

# Combine them into one DataFrame
labels_df = pd.concat([train, dev, test], ignore_index=True)[['Participant_ID', 'PHQ8_Score', 'PHQ8_Binary']].sort_values('Participant_ID').reset_index(drop=True)

# Save to file if needed
labels_df.to_csv('daicwoz_all_labels.csv', index=False)

"""## *Participant texts (Statements)*

Now, let's extract the statements.
"""

import zipfile
import os

# Adjust the filename to match your uploaded zip
zip_path = 'Transcripts.zip'
extract_dir = 'transcripts/'       # Folder to extract contents into

# Unzip the file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

# Confirm extraction
print("Files extracted to:", extract_dir)
print("Sample files size:", len(os.listdir(extract_dir)))

participant_texts = []

for filename in os.listdir(extract_dir):
    if filename.endswith('_TRANSCRIPT.csv'):
        participant_id = int(filename.split('_')[0])
        path = os.path.join(extract_dir, filename)

        try:
            df = pd.read_csv(path, sep='\t', names=['start_time', 'stop_time', 'speaker', 'value'], engine='python', quoting=3)
            participant_lines = df[df['speaker'] == 'Participant']['value']
            full_text = ' '.join(str(t) for t in participant_lines if pd.notnull(t))

            participant_texts.append({'Participant_ID': participant_id, 'Text': full_text})

        except Exception as e:
            print(f"Error with file {filename}: {e}")

texts_df = pd.DataFrame(participant_texts).sort_values('Participant_ID').reset_index(drop=True)
texts_df.head() # Hidden results due to sensible data

"""## *Merge Texts with Labels*"""

labels_df = pd.read_csv('daicwoz_all_labels.csv')

final_df = pd.merge(texts_df, labels_df, on='Participant_ID', how='inner')
final_df = final_df[['Text', 'PHQ8_Binary']].rename(columns={
    'Text': 'statement',
    'PHQ8_Binary': 'status'
})
final_df['status'] = final_df['status'].map({1: 'Depression', 0: 'Normal'})

final_df.to_csv('daicwoz_depression.csv', index=False)

final_df[final_df['status']=='Normal'].head() # Hidden results due to sensible data
final_df[final_df['status']=='Depression'].head() # Hidden results due to sensible data

"""# **EDA**

## **Initial Data Examination:**

Display top 5 rows of the dataset:
"""

df= pd.read_csv('daicwoz_depression.csv')
df.head() # Hidden results due to sensible data

"""Summary of the dataset:"""

df.info()

"""Dataset description:"""

df.describe() # Hidden results due to sensible data

"""Basic inspection of the dataset:"""

shape = df.shape
columns = df.columns.tolist()
dtypes = df.dtypes
missing_values = df.isnull().sum()
duplicates = df.duplicated().sum()

print("Dataset's shape: \n" + str(shape) +
      "\n\nDataset's columns: \n" + str(columns) +
      "\n\nData types: \n" + str(dtypes) +
      "\n\nMissing values:\n " + str(missing_values) +
      "\n\nDuplicates: \n" + str(duplicates))

"""## **Frequency distribution of our Target**"""

status_counts = df['status'].value_counts()

# Generate colors from the chosen colormap
cmap = plt.get_cmap('OrRd_r')  # or 'copper', 'YlOrBr', etc.
colors = cmap(np.linspace(0.3, 0.9, len(status_counts)))

plt.figure(figsize=(10, 6))
status_counts.plot(kind='bar', color=colors)

plt.title('Frequency Distribution of Status')
plt.xlabel('Status')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.grid(axis='y')

plt.show()

status_counts

"""## **Text Length Analysis of statement**"""

df = df.copy()
# Calculate text length features safely
df['word_count'] = df['statement'].apply(lambda x: len(x.split()))

# Descriptive statistics of these new features
length_stats = df[['word_count']].describe()

length_stats

# Set up the plot
plt.figure(figsize=(10, 6))

# Histogram of word counts
sns.histplot(df['word_count'], bins=50, kde=True)

# Customize the plot
plt.title('Distribution of Word Counts')
plt.xlabel('Word Count')
plt.ylabel('Frequency')

# Show the plot
plt.tight_layout()
plt.show()

"""**Word Count:** Shows a right-skewed distribution â€” most statements are around 1,000 characters, which matches the max of the kaggle dataset. This makes sense, considering it's not posts anymore."""

plt.figure(figsize=(10, 6))

sns.boxplot(x='status', y='word_count', data=df)

# Customize the plot
plt.title('Word Counts by Status')
plt.xlabel('Status')
plt.ylabel('Word Count')
plt.xticks(rotation=45)

# Show the plot
plt.tight_layout()
plt.show()

"""Much more stabilised.

# **Dataset Preprocessing**
"""

!pip install contractions
import contractions

nltk.download('stopwords')
nltk.download('punkt')

def preprocess_text(text):
    text = contractions.fix(text)  # Expandir contracciones (e.g. I'm -> I am)
    text = re.sub(r'<.*?>+', '', text)  # Remove HTML tags
    text = re.sub(r'http[s]?://\S+', '', text)     # Remove URLs
    text = re.sub(r'@\w+', '', text)  # Remove handles (that start with '@')
    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)  # Remove punctuation
    text = re.sub(r'\n', '', text)  # Remove newlines
    text = re.sub(r'\[.*?\]', '', text)  # Remove text in square brackets
    text = re.sub(r'\w*\d\w*', '', text)  # Remove words containing numbers
    text = text.lower()  # Lowercase text

    return text

df['cleaned_statement'] = df['statement'].apply(lambda x: preprocess_text(x))

"""Then, we lemmatize:"""

nlp = spacy.load('en_core_web_sm')

def lemmatize_sentence(sentence):
    doc = nlp(sentence)
    return " ".join([token.lemma_ for token in doc if not token.is_punct and not token.is_space])

df['cleaned_statement'] = df['cleaned_statement'].apply(lemmatize_sentence)

"""We tokenize and remove stop words."""

nltk.download('punkt_tab')

# Tokenization and Stopwords Removal
stop_words = set(stopwords.words('english'))

def remove_stopwords_and_tokenize(text):
    tokens = word_tokenize(text)
    filtered_tokens = [word for word in tokens if word.lower() not in stop_words and len(word) > 2]
    cleaned_text = ' '.join(filtered_tokens)
    return pd.Series([cleaned_text, filtered_tokens])

df[['cleaned_statement', 'tokens']] = df['cleaned_statement'].apply(remove_stopwords_and_tokenize)

df = df[df['cleaned_statement'].str.strip().astype(bool)]  # remove empty or spaces

from google.colab import files

# Guardar el DataFrame como archivo CSV
df.to_csv('clean_DAICWOZ.csv', index=False,)

# Descargar el archivo CSV
files.download('clean_DAICWOZ.csv')

"""## **Word Clouds**"""

# Function to plot word cloud for a specific status
def plot_wordcloud_for_status(status_name):
    text = " ".join(df[df['status'] == status_name]['cleaned_statement'].astype(str))
    wordcloud = WordCloud(width=600, height=300, background_color='white', stopwords='english', max_words=85).generate(text)
    plt.figure(figsize=(12, 4))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(f"Word Cloud for {status_name}", fontsize=16)
    plt.axis('off')
    plt.tight_layout()
    plt.show()

# Generate for 'Anxiety'
plot_wordcloud_for_status("Depression")

# Generate for 'Normal'
plot_wordcloud_for_status("Normal")