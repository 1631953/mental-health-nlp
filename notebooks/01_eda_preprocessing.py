# -*- coding: utf-8 -*-
"""eda_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mGXSIVLKj55mvspK4hdzje3NmLlvKPeR
"""

# Comment libraries used
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import seaborn as sns
import scipy.stats as stats
from collections import defaultdict
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
import re
import os
import spacy

df = pd.read_csv("Combined Data.csv")

"""# **EDA**

## **Initial Data Examination:**

Display top 5 rows of the dataset:
"""

df.head()

"""Display last 5 rows of the dataset:"""

df.tail()

"""Drop Index column:"""

df.drop(columns=['Unnamed: 0'],inplace=True)

"""Summary of the dataset:"""

df.info()

"""Dataset description:"""

df.describe()

"""Basic inspection of the dataset:"""

shape = df.shape
columns = df.columns.tolist()
dtypes = df.dtypes
missing_values = df.isnull().sum()
duplicates = df.duplicated().sum()

print("Dataset's shape: \n" + str(shape) +
      "\n\nDataset's columns: \n" + str(columns) +
      "\n\nData types: \n" + str(dtypes) +
      "\n\nMissing values:\n " + str(missing_values) +
      "\n\nDuplicates: \n" + str(duplicates))

"""***Dataset Summary:***

Rows: 53,043

Columns: 3 ⟶ Unnamed: 0 (dropped), statement, status

Missing Values:
- statement: 362 missing
- Others: No missing values

Duplicates:
- 1588 duplicate rows

Unique Values:
- statement: 51,073 unique entries, most common = "what do you mean?" (22 occurrences)
- status: 7 unique categories, most frequent = "Normal" (16,351 times).

**Handle null values:**
"""

df.dropna(inplace = True)
df.isna().sum()

"""**Handle duplicates:**

First, we deduplicate exact rows to avoid skewing model training and metrics due to repetition:
"""

# Remove fully duplicated rows (same statement + same status)
df_deduplicated = df.drop_duplicates(subset=['statement', 'status'])

# Show how many rows were removed
original_count = df.shape[0]
deduplicated_count = df_deduplicated.shape[0]
removed_count = original_count - deduplicated_count

removed_count

"""Now, we analyze the most frequent statements:"""

statement_counts = df_deduplicated['statement'].value_counts(dropna=True)

# Top 10 most common statements
top_statements = statement_counts.head(10)
top_statements

"""We remove the rows with the statement "#NAME?", as this statement, while being labeled as various mental health conditions, is clearly unrelated to any of them."""

df_cleaned = df_deduplicated[df_deduplicated['statement'] != '#NAME?']

"""The duplicate statements we have left have different labels.  Such inconsistencies can severely affect text classification accuracy. We remove these to preserve dataset integrity."""

# Find duplicated statements
duplicate_statements = df_cleaned[df_cleaned.duplicated('statement', keep=False)]

# Identify those with conflicting statuses
statement_status_variation = duplicate_statements.groupby('statement')['status'].nunique()
conflicting_statements = statement_status_variation[statement_status_variation > 1]

# Extract and group by statement
conflicting_entries = duplicate_statements[duplicate_statements['statement'].isin(conflicting_statements.index)]
grouped_conflicting = conflicting_entries.sort_values('statement').groupby('statement')

# Create a grouped view with all status entries for each conflicting statement
grouped_data = grouped_conflicting.agg({'status': lambda x: list(x)}).reset_index()

# Remove conflictive statements
df_consistent = df_cleaned[~df_cleaned['statement'].isin(conflicting_statements.index)]

print("Conflicting Entries Removed:\n", grouped_data)

"""We remove Personality disorder and Stress, due to underrepresentation"""

df_consistent = df_consistent[~df_consistent['status'].isin(['Personality disorder', 'Stress'])]

"""## **Frequency distribution of our Target**"""

status_counts = df_consistent['status'].value_counts()

# Generate colors from the chosen colormap
cmap = plt.get_cmap('OrRd_r')  # or 'copper', 'YlOrBr', etc.
colors = cmap(np.linspace(0.3, 0.9, len(status_counts)))

plt.figure(figsize=(10, 6))
status_counts.plot(kind='bar', color=colors)

plt.title('Frequency Distribution of Status')
plt.xlabel('Status')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.grid(axis='y')

plt.show()

status_counts

"""**Most Frequent:** "Normal" (16,037 entries) — ~30% of data.

**Heavily Represented Conditions:** Depression and Suicidal collectively account for almost 50% of the dataset.

**Rare Category:** Personality disorder (only ~2% of total entries).

There's an obvious multiclass imbalance that will be handled later.

## **Text Length Analysis of statement**
"""

df_consistent = df_consistent.copy()
# Calculate text length features safely
df_consistent['char_length'] = df_consistent['statement'].apply(len)
df_consistent['word_count'] = df_consistent['statement'].apply(lambda x: len(x.split()))

# Descriptive statistics of these new features
length_stats = df_consistent[['char_length', 'word_count']].describe()

length_stats

# Set up the plots
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Histogram of character lengths
sns.histplot(df_consistent['char_length'], bins=50, kde=True, ax=axes[0])
axes[0].set_title('Distribution of Character Lengths')
axes[0].set_xlabel('Character Length')
axes[0].set_ylabel('Frequency')

# Histogram of word counts
sns.histplot(df_consistent['word_count'], bins=50, kde=True, ax=axes[1])
axes[1].set_title('Distribution of Word Counts')
axes[1].set_xlabel('Word Count')
axes[1].set_ylabel('Frequency')

plt.tight_layout()
plt.show()

"""**Character Lengths:** Shows a right-skewed distribution — most statements are under 1,000 characters, but a few outliers stretch beyond 30,000. ***Some outliers may be removed***

**Word Count:** Similarly skewed — most statements fall under 200 words, but a few reach up to 6,300 words.
"""

# Set up the plots
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Boxplot of character lengths by status
sns.boxplot(x='status', y='char_length', data=df_consistent, ax=axes[0])
axes[0].set_title('Character Lengths by Status')
axes[0].set_xlabel('Status')
axes[0].set_ylabel('Character Length')
axes[0].tick_params(axis='x', rotation=45)

# Boxplot of word counts by status
sns.boxplot(x='status', y='word_count', data=df_consistent, ax=axes[1])
axes[1].set_title('Word Counts by Status')
axes[1].set_xlabel('Status')
axes[1].set_ylabel('Word Count')
axes[1].tick_params(axis='x', rotation=45)

"""**Character Lengths by Status:** "Normal" tends to have shorter messages on average, while categories like "Suicidal", "Bipolar", and "Depression" show a wider spread and higher median lengths.

**Word Counts by Status**: Mirroring the pattern of character lengths, emotional or clinical categories tend to have longer statements.

Usually, word_count is better than char_length when modeling language behavior. **We may add char_length later if needed or a combination of both as the average word length (char_length / word_count).**
"""

df_consistent.drop(columns=['char_length'],inplace=True)
df_consistent

"""#**Preprocessing**

## **Preprocessing Steps**

We first clean the dataset by expanding contractions, removing HTML tags, URLs, handles, punctuation, newline characters, text in square brackets and words containing numbers. We also convert all text to lowercase.
"""

!pip install contractions
import contractions

nltk.download('stopwords')
nltk.download('punkt')

def preprocess_text(text):
    text = contractions.fix(text)  # Expandir contracciones (e.g. I'm -> I am)
    text = re.sub(r'<.*?>+', '', text)  # Remove HTML tags
    text = re.sub(r'http[s]?://\S+', '', text)     # Remove URLs
    text = re.sub(r'@\w+', '', text)  # Remove handles (that start with '@')
    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)  # Remove punctuation
    text = re.sub(r'\n', '', text)  # Remove newlines
    text = re.sub(r'\[.*?\]', '', text)  # Remove text in square brackets
    text = re.sub(r'\w*\d\w*', '', text)  # Remove words containing numbers
    text = text.lower()  # Lowercase text

    return text

df_consistent['cleaned_statement'] = df_consistent['statement'].apply(lambda x: preprocess_text(x))

"""Then, we lemmatize:"""

nlp = spacy.load('en_core_web_sm')

def lemmatize_sentence(sentence):
    doc = nlp(sentence)
    return " ".join([token.lemma_ for token in doc if not token.is_punct and not token.is_space])

df_consistent['cleaned_statement'] = df_consistent['cleaned_statement'].apply(lemmatize_sentence)

"""We tokenize and remove stop words."""

nltk.download('punkt_tab')

# Tokenization and Stopwords Removal
stop_words = set(stopwords.words('english'))

def remove_stopwords_and_tokenize(text):
    tokens = word_tokenize(text)
    filtered_tokens = [word for word in tokens if word.lower() not in stop_words and len(word) > 2]
    cleaned_text = ' '.join(filtered_tokens)
    return pd.Series([cleaned_text, filtered_tokens])

df_consistent[['cleaned_statement', 'tokens']] = df_consistent['cleaned_statement'].apply(remove_stopwords_and_tokenize)

df_consistent = df_consistent[df_consistent['cleaned_statement'].str.strip().astype(bool)]  # remove empty or spaces

from google.colab import files

# Guardar el DataFrame como archivo CSV
df_consistent.to_csv('df_PostPreprocessing_5classes.csv', index=False,)

# Descargar el archivo CSV
files.download('df_PostPreprocessing_5classes.csv')

"""## **Word Clouds**"""

# Function to plot word cloud for a specific status
def plot_wordcloud_for_status(status_name):
    text = " ".join(df_consistent[df_consistent['status'] == status_name]['cleaned_statement'].astype(str))
    wordcloud = WordCloud(width=600, height=300, background_color='white', stopwords='english', max_words=85).generate(text)
    plt.figure(figsize=(12, 4))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(f"Word Cloud for {status_name}", fontsize=16)
    plt.axis('off')
    plt.tight_layout()
    plt.show()

# Generate for 'Anxiety'
plot_wordcloud_for_status("Anxiety")

# Generate for 'Normal'
plot_wordcloud_for_status("Normal")

# Generate for 'Depression'
plot_wordcloud_for_status("Depression")

# Generate for 'Suicidal'
plot_wordcloud_for_status("Suicidal")

# Generate for 'Bipolar'
plot_wordcloud_for_status("Bipolar")

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def plot_all_wordclouds(status_list):
    fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 8))
    fig.subplots_adjust(hspace=0.4, wspace=0.3)

    for idx, status_name in enumerate(status_list):
        row, col = divmod(idx, 3)
        ax = axes[row, col]
        text = " ".join(df_consistent[df_consistent['status'] == status_name]['cleaned_statement'].astype(str))
        wordcloud = WordCloud(width=600, height=300, background_color='white', stopwords='english', max_words=85).generate(text)
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.set_title(f"Word Cloud for {status_name}", fontsize=14)
        ax.axis('off')

    # Hide empty subplot (if only 5 statuses)
    if len(status_list) == 5:
        axes[1, 2].axis('off')  # Hide the 6th subplot
        # Move the last word cloud (index 4) to center bottom
        axes[1, 1].imshow(
            WordCloud(width=600, height=300, background_color='white', stopwords='english', max_words=85).generate(
                " ".join(df_consistent[df_consistent['status'] == status_list[4]]['cleaned_statement'].astype(str))
            ), interpolation='bilinear'
        )
        axes[1, 1].set_title(f"Word Cloud for {status_list[4]}", fontsize=14)
        axes[1, 1].axis('off')
        axes[1, 0].axis('off')

    plt.tight_layout()
    plt.show()

plot_all_wordclouds(['Anxiety', 'Bipolar', 'Depression', 'Normal', 'Suicidal'])