# LANGUAGE ANALYSIS
# -*- coding: utf-8 -*-
"""02_language_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mSAKrtae9OYS--ON29XWnP0RM2VWjyTd
"""

# Comment libraries used
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import seaborn as sns
import scipy.stats as stats
from collections import defaultdict
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
import re
import os
import spacy

"""#**Language Exploratory Analysis**"""

df = pd.read_csv("df_PostPreprocessing_5classes.csv")

"""## **Topic Modeling (LDA)**"""

# Uninstall both gensim and numpy to ensure a clean state
!pip uninstall -y gensim numpy

# Reinstall numpy first to ensure a compatible version is available
!pip install numpy==1.24.4

# Reinstall gensim, which should now pick up the installed numpy version
!pip install gensim

!pip install pyLDAvis

import gensim
from gensim import corpora
from gensim.models import LdaModel
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
import pyLDAvis.gensim_models
import pyLDAvis
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab')

def lda_by_category(category, num_topics=5, num_words=5, save_html=True, show_vis=False):
    print(f"\n=== Topics for Category: {category} ===")

    # Filter data for this category
    category_docs = df[df['status'] == category]['tokens']

    # Clean and ensure token format !!!
    processed_docs = []
    for doc in category_docs:
        if isinstance(doc, str):
            processed_docs.append(word_tokenize(doc))
        elif isinstance(doc, list):
            processed_docs.append([str(item) for item in doc if isinstance(item, (str, int, float))])
    processed_docs = [doc for doc in processed_docs if doc]
    if not processed_docs:
        print(f"No valid documents found for category: {category}")
        return

    # Create Dictionary and Corpus
    dictionary = corpora.Dictionary(processed_docs)
    corpus = [dictionary.doc2bow(text) for text in processed_docs]

    # Filter extremes to improve topic quality
    dictionary.filter_extremes(no_below=5, no_above=0.5)
    corpus = [dictionary.doc2bow(text) for text in processed_docs]

    # Train LDA model
    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42, passes=10)

    # Print topics
    for idx, topic in lda_model.print_topics(num_topics=num_topics, num_words=num_words):
        print(f"Topic {idx + 1}: {topic}")

    # Create visualization
    vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)

    # Save HTML file
    if save_html:
        safe_name = category.lower().replace(" ", "_")
        pyLDAvis.save_html(vis, f"{safe_name}_lda.html")

    # Optionally show in notebook
    if show_vis:
        pyLDAvis.display(vis)

lda_by_category('Anxiety', num_topics=4) # MAX 4

lda_by_category('Normal', num_topics=5) # MAX 5

lda_by_category('Depression', num_topics=4) # MAX 4

lda_by_category('Suicidal', num_topics=3) # MAX 3

lda_by_category('Bipolar', num_topics=4) # MAX 4

"""## **Text Representation Techniques - Frequency Analysis and Embedding Extraction**

### **Frequency Analysis**

#### **CountVectorizer**
"""

# Initialize a CountVectorizer for unigrams (top keywords)
vectorizer = CountVectorizer(stop_words='english', max_features=1000, max_df=1.0, ngram_range=(1, 3))
# Dictionary to store top keywords per category
top_keywords_per_status = {}

# Loop through each status category
for category in df['status'].unique():
    # Filter statements by status
    texts = df[df['status']== category]['cleaned_statement']

    # Fit and transform text to get word frequencies
    X = vectorizer.fit_transform(texts)
    sum_words = X.sum(axis=0)

    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)

    top_keywords_per_status[category] = words_freq

# Display top keywords per status
# import pandas as pd
# keyword_df = pd.DataFrame.from_dict(top_keywords_per_status, orient='index')
# keyword_df

import matplotlib.pyplot as plt

def plot_bow_bars(bow_data, top_n=10):
    num_categories = len(bow_data)
    cols = 3
    rows = (num_categories + 1) // cols  # Ceiling division

    fig, axes = plt.subplots(rows, cols, figsize=(15, 4 * rows))
    axes = axes.flatten()

    for i, (category, word_scores) in enumerate(bow_data.items()):
        words = [w for w, s in word_scores[:top_n]]
        scores = [s for w, s in word_scores[:top_n]]

        ax = axes[i]
        ax.barh(words[::-1], scores[::-1], color='skyblue')
        ax.set_title(f"{category}", fontsize=12)
        ax.set_xlabel("Word Count using BoW")
        ax.grid(axis='x', linestyle='--', alpha=0.6)

    # Hide unused subplots
    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])

    plt.tight_layout()
    plt.show()

# Run the function
plot_bow_bars(top_keywords_per_status)

"""The table above shows the top 10 most frequent words per mental health status category. Here's a quick summary:
*   Many emotionally charged words like "feel" "want", "like", "know", "make" and "think" appear across nearly all categories. (These may not seem emotional but, after analyzing the dataset, we know they are used to express feelings, for example, make,--> make somone anxious/ make someone's heart ache....). This suggests there might be a shared linguistic pattern in expressing emotional states, regardless of diagnosis. (Moreover, the word "time" appears among all disorders, indicating....)
  
  
*   Still, by analyzing each mental health category's high-fequency words, reflection of unique concerns can be studied:
  * **Anxiety:**

      The keyword *anxiety* itself is prominent, as well as "time", "day", "really", "start" and "bad", which could indicate anticipatory or racing thoughts, typical in anxious expressions.

  * **Normal:**

      While the emotionally expressive common keywords "want", "like", "know" and "good" still show up, they do with a significantly lower frequency.. This suggests a less intense emotional weight.

  * **Depression:**
      
      While terms like "feel" and "want" dominate, other keywords such as "life", "know" and "want" also appear frequently. They could indicate themes of self-reflection, existential concern, and emotional struggle.

  * **Suicidal:**

      Highly emotional words like "want" and "feel" are heavily represented, along with "life", "people" and "try", potentially reflecting social disconnect and desperation.

  * **Bipolar:**

      The appearance of "bipolar", "feel", "like", "year" and "know" may point to identity reflection and emotional fluctuation, which are common among bipolar narratives.

#### **TF-IDF**
"""

from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

# Dictionary to store top TF-IDF keywords per category
top_tfidf_per_status = {}

# Initialize a TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)

# Loop through each status category
for category in df['status'].unique():
    # Filter statements by category
    texts = df[df['status'] == category]['cleaned_statement']

    # Fit TF-IDF on this subset
    X = tfidf_vectorizer.fit_transform(texts)

    # Sum the TF-IDF weights for each term
    sums = X.sum(axis=0)

    # Map words to their summed TF-IDF scores
    tfidf_scores = [(word, sums[0, idx]) for word, idx in tfidf_vectorizer.vocabulary_.items()]
    tfidf_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)

    # Store top 10 for this category
    top_tfidf_per_status[category] = tfidf_scores[:10]

# Display as DataFrame
# tfidf_df = pd.DataFrame.from_dict(top_tfidf_per_status, orient='index')
# tfidf_df.columns = [f"Word {i+1}" for i in range(tfidf_df.shape[1])]
# tfidf_df

import matplotlib.pyplot as plt

def plot_tfidf_bars(tfidf_data, top_n=10):
    num_categories = len(tfidf_data)
    cols = 3
    rows = (num_categories + 1) // cols  # Ceiling division

    fig, axes = plt.subplots(rows, cols, figsize=(15, 4 * rows))
    axes = axes.flatten()

    for i, (category, word_scores) in enumerate(tfidf_data.items()):
        words = [w for w, s in word_scores[:top_n]]
        scores = [s for w, s in word_scores[:top_n]]

        ax = axes[i]
        ax.barh(words[::-1], scores[::-1], color='skyblue')
        ax.set_title(f"{category}", fontsize=12)
        ax.set_xlabel("TF-IDF Score")
        ax.grid(axis='x', linestyle='--', alpha=0.6)

    # Hide unused subplots
    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])

    plt.tight_layout()
    plt.show()

# Run the function
plot_tfidf_bars(top_tfidf_per_status)

"""While BoW's dominant words are still present, TF-IDF brings up more disorder-specific words:

  * **Anxiety:**
  
  TF-IDF brings up words like *restless* and *pain*, which are more specific symptoms of anxiety.

  
  * *Normal*:
  
  The overlap is high. TF-IDF confirms that Normal posts contain less emotionally distinctive vocabulary and show neutraal, casual vocabulary.

  
  * Depression:
  
  TF-IDF identifies "depression", "life", and "people" as particularly relevant. BoW also captures some of them, but less predominantly.

  
  * **Suicidal:**
  
  TF-IDF introduces "die" and "kill", highly relevant and concerning terms in suicidal ideation. **TF-IDF surfaces clinical or high-risk indicators.**

  * Bipolar:

  Again, huge overlap and thematic consistency. TF-IDF highlights "bipolar".

### **Word Embeddings**

BoW and TF-IDF represent words or documents based on exact token occurrences. They don't have any understanding of meaning or semantics. To them, "good" and "great" are unrelated unless they co-occur often. This means context is not captured and each word is treated independently.

Meanwhile, embeddings go beyond surface-level counting. We have:

* **Traditional, static embeddings (Word2Vec, GloVe, FastText),** which:

  - Capture *semantic similarity* ("king" and "queen" are close in vector space), *syntactic patterns* ("walk" and "walked" have related vectors) and *distributional meaning* ("You shall know a word by the company it keeps." - *John Rupert Firth*).

  - Do not capture *contextual meaning* (The word "bank" has the same vector in both "river bank" and "bank loan") nor polysemy.



* **Contextual Embeddings (e.g., BERT, RoBERTa, GPT embeddings)**, which do understand context in specific sentences. These generate different vectors for the same word depending on the sentence it's in. (For example, "He went to the bank to fish" and "She went to the bank to deposit cash" will produce different vectors for "bank").

#### **Word2Vec Embeddings**
"""

!pip install --force-reinstall numpy==1.24.4
!pip install --upgrade gensim

import gensim
from gensim.models import Word2Vec
from tqdm import tqdm


# Train Word2Vec model
w2v_model = Word2Vec(df['tokens'], vector_size=100, window=5, min_count=2, workers=4, sg=1, epochs=20)

# Example: Get vector for a word
# print("Vector for 'anxiety':", w2v_model.wv['anxiety'])

# Save the model
w2v_model.save("w2v_model.bin")
